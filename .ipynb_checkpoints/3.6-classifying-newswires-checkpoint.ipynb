{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\program_files\\miniconda3\\envs\\dl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
    "But what happens when you have more than two classes? \n",
    "\n",
    "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
    "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 25s 12us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 4s 8us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    " \n",
    "这个主题分类问题看起来非常类似于我们以前的电影评论分类问题：在这两种情况下，我们正在尝试分类短片段的文本。然而，这里有一个新的约束：输出类的数量从2增加到46，即输出空间的维数要大得多。    \n",
    "在我们所使用的密集层堆栈中，每个层只能访问前一层输出中存在的信息。如果一层删除了与分类问题相关的信息，那么这些信息就永远不能被后面的层所恢复：每一层都有可能成为“信息瓶颈”。在我们前面的例子中，我们使用了16个维度的中间层，但是16维空间可能太有限，无法学会分离46个不同的类：这样的小层可以充当信息瓶颈，永久地丢弃相关信息。    \n",
    "由于这个原因，我们将使用具有更多单元的层。这里使用用64个单元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 331us/step - loss: 2.5332 - acc: 0.4960 - val_loss: 1.7225 - val_acc: 0.6110\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 210us/step - loss: 1.4488 - acc: 0.6869 - val_loss: 1.3502 - val_acc: 0.7070\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 219us/step - loss: 1.0999 - acc: 0.7642 - val_loss: 1.1732 - val_acc: 0.7420\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 0.8726 - acc: 0.8151 - val_loss: 1.0784 - val_acc: 0.7580\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 0.7058 - acc: 0.8473 - val_loss: 0.9854 - val_acc: 0.7830\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 0.5691 - acc: 0.8800 - val_loss: 0.9404 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.4618 - acc: 0.9033 - val_loss: 0.9073 - val_acc: 0.8010\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.3731 - acc: 0.9219 - val_loss: 0.9348 - val_acc: 0.7880\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.3057 - acc: 0.9306 - val_loss: 0.8907 - val_acc: 0.8060\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 0.2556 - acc: 0.9409 - val_loss: 0.9058 - val_acc: 0.8090\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.2199 - acc: 0.9469 - val_loss: 0.9174 - val_acc: 0.8110\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 202us/step - loss: 0.1884 - acc: 0.9506 - val_loss: 0.9035 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.1706 - acc: 0.9525 - val_loss: 0.9329 - val_acc: 0.8100\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 205us/step - loss: 0.1541 - acc: 0.9553 - val_loss: 0.9654 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.1392 - acc: 0.9560 - val_loss: 0.9696 - val_acc: 0.8160\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 0.1312 - acc: 0.9558 - val_loss: 1.0270 - val_acc: 0.8010\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.1220 - acc: 0.9580 - val_loss: 1.0227 - val_acc: 0.7970\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.1197 - acc: 0.9578 - val_loss: 1.0452 - val_acc: 0.8050\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 208us/step - loss: 0.1137 - acc: 0.9593 - val_loss: 1.0958 - val_acc: 0.7980\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 219us/step - loss: 0.1107 - acc: 0.9594 - val_loss: 1.0701 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNX5x/HPw74KCFoQhIA7IEKMCIKCS/251LVWRahi9YfaulS7SHG3pXWrUtRaaZVaSaFWfy5V1NqKxaVFAwIKSEFBjCIElE1QCTy/P87NZAiTZEJyM5Pk+3697mvu3HvmzjM3k/vMPefcc83dERERAWiU6QBERCR7KCmIiEiCkoKIiCQoKYiISIKSgoiIJCgpiIhIgpKC1Cgza2xmm8yse02WzSQz29fMarzvtpkdZ2bLk54vNrMj0ym7C+/1BzMbt6uvr2C7vzCzP9b0diVzmmQ6AMksM9uU9LQV8BWwLXp+ibvnV2V77r4NaFPTZRsCdz+gJrZjZhcDo9x9eNK2L66JbUv9p6TQwLl74qAc/RK92N3/UV55M2vi7sW1EZuI1D5VH0mFouqBv5jZVDPbCIwys8Fm9h8zW2dmK81sopk1jco3MTM3s5zo+ZRo/fNmttHM/m1mPataNlp/opn918zWm9m9Zva6mY0uJ+50YrzEzJaa2edmNjHptY3N7B4zW2tm7wMnVLB/rjezaWWW3W9md0fzF5vZoujzvB/9ii9vW4VmNjyab2Vmj0axLQAOTfG+H0TbXWBmp0bLDwbuA46MqubWJO3bm5Nef2n02dea2VNm1iWdfVMZMzs9imedmb1sZgckrRtnZp+Y2QYzey/psw4ysznR8lVmdme67ycxcHdNmnB3gOXAcWWW/QL4GjiF8COiJXAYcDjhTLMX8F/g8qh8E8CBnOj5FGANkAc0Bf4CTNmFsnsCG4HTonXXAFuB0eV8lnRifBpoB+QAn5V8duByYAHQDegIzAz/KinfpxewCWidtO3VQF70/JSojAHHAFuAftG644DlSdsqBIZH83cBrwAdgB7AwjJlzwa6RH+T86IYvhGtuxh4pUycU4Cbo/njoxj7Ay2A3wIvp7NvUnz+XwB/jOYPiuI4JvobjYv2e1OgD/Ah0Dkq2xPoFc2/BYyI5tsCh2f6f6EhTzpTkHS85u5/c/ft7r7F3d9y91nuXuzuHwCTgGEVvP5xdy9w961APuFgVNWy3wLmuvvT0bp7CAkkpTRj/JW7r3f35YQDcMl7nQ3c4+6F7r4WuK2C9/kAeJeQrAC+Caxz94Jo/d/c/QMPXgb+CaRsTC7jbOAX7v65u39I+PWf/L6PufvK6G/yZ0JCz0tjuwAjgT+4+1x3/xIYCwwzs25JZcrbNxU5F3jG3V+O/ka3AbsRknMxIQH1iaogl0X7DkJy38/MOrr7RneflebnkBgoKUg6Pkp+YmYHmtlzZvapmW0AbgU6VfD6T5PmN1Nx43J5ZfdKjsPdnfDLOqU0Y0zrvQi/cCvyZ2BENH8eIZmVxPEtM5tlZp+Z2TrCr/SK9lWJLhXFYGajzWxeVE2zDjgwze1C+HyJ7bn7BuBzoGtSmar8zcrb7nbC36iruy8GfkT4O6yOqiM7R0UvBHoDi83sTTM7Kc3PITFQUpB0lO2O+SDh1/G+7r4bcCOheiROKwnVOQCYmbHjQays6sS4Etg76XllXWb/AhwX/dI+jZAkMLOWwOPArwhVO+2Bv6cZx6flxWBmvYAHgMuAjtF230vabmXdZz8hVEmVbK8toZrq4zTiqsp2GxH+Zh8DuPsUdx9CqDpqTNgvuPtidz+XUEX4a+AJM2tRzVhkFykpyK5oC6wHvjCzg4BLauE9nwVyzewUM2sCXAXsEVOMjwE/NLOuZtYRuLaiwu6+CngNmAwsdvcl0armQDOgCNhmZt8Cjq1CDOPMrL2F6zguT1rXhnDgLyLkx4sJZwolVgHdShrWU5gKXGRm/cysOeHg/Kq7l3vmVYWYTzWz4dF7/4TQDjTLzA4ys6Oj99sSTdsIH+C7ZtYpOrNYH3227dWMRXaRkoLsih8BFxD+4R8k/FKOVXTgPQe4G1gL7AO8TbiuoqZjfIBQ9/8OoRH08TRe82dCw/Gfk2JeB1wNPElorD2LkNzScRPhjGU58Dzwp6TtzgcmAm9GZQ4EkuvhXwKWAKvMLLkaqOT1LxCqcZ6MXt+d0M5QLe6+gLDPHyAkrBOAU6P2hebAHYR2oE8JZybXRy89CVhkoXfbXcA57v51deORXWOhalakbjGzxoTqirPc/dVMxyNSX+hMQeoMMzvBzNpFVRA3EHq0vJnhsETqFSUFqUuGAh8QqiBOAE539/Kqj0RkF6j6SEREEnSmICIiCXVuQLxOnTp5Tk5OpsMQEalTZs+evcbdK+rGDdTBpJCTk0NBQUGmwxARqVPMrLIr8wFVH4mISBIlBRERSVBSEBGRhDrXpiAitWvr1q0UFhby5ZdfZjoUSUOLFi3o1q0bTZuWN/RVxZQURKRChYWFtG3blpycHMLgtJKt3J21a9dSWFhIz549K39BCg2i+ig/H3JyoFGj8JhfpVvRizRsX375JR07dlRCqAPMjI4dO1brrK7enynk58OYMbB5c3j+4YfhOcDIao8LKdIwKCHUHdX9W9X7M4XrritNCCU2bw7LRURkR7ElBTPb28xmmNkiM1tgZlelKDPczNab2dxourGm41ixomrLRSS7rF27lv79+9O/f386d+5M165dE8+//jq92y5ceOGFLF68uMIy999/P/k1VLc8dOhQ5s6dWyPbqm1xVh8VAz9y9znR7f5mm9lL7r6wTLlX3f1bcQXRvXuoMkq1XERqXn5+OBNfsSL8n40fX72q2o4dOyYOsDfffDNt2rThxz/+8Q5l3B13p1Gj1L9zJ0+eXOn7/OAHP9j1IOuR2M4U3H2lu8+J5jcCi6j4nrqxGD8eWrXacVmrVmG5iNSskja8Dz8E99I2vDg6dyxdupS+ffty6aWXkpuby8qVKxkzZgx5eXn06dOHW2+9NVG25Jd7cXEx7du3Z+zYsRxyyCEMHjyY1atXA3D99dczYcKERPmxY8cycOBADjjgAN544w0AvvjiC7797W9zyCGHMGLECPLy8io9I5gyZQoHH3wwffv2Zdy4cQAUFxfz3e9+N7F84sSJANxzzz307t2bQw45hFGjRtX4PktHrbQpmFkOMIAdbxlYYrCZzTOz582sTzmvH2NmBWZWUFRUVKX3HjkSJk2CHj3ALDxOmqRGZpE41HYb3sKFC7nooot4++236dq1K7fddhsFBQXMmzePl156iYULy1ZMwPr16xk2bBjz5s1j8ODBPPzwwym37e68+eab3HnnnYkEc++999K5c2fmzZvH2LFjefvttyuMr7CwkOuvv54ZM2bw9ttv8/rrr/Pss88ye/Zs1qxZwzvvvMO7777L+eefD8Add9zB3LlzmTdvHvfdd181986uiT0pmFkb4Angh+6+oczqOUAPdz8EuBd4KtU23H2Su+e5e94ee1Q6yN9ORo6E5cth+/bwqIQgEo/absPbZ599OOywwxLPp06dSm5uLrm5uSxatChlUmjZsiUnnngiAIceeijLly9Pue0zzzxzpzKvvfYa5557LgCHHHIIffqk/B2bMGvWLI455hg6depE06ZNOe+885g5cyb77rsvixcv5qqrruLFF1+kXbt2APTp04dRo0aRn5+/yxefVVesScHMmhISQr67/1/Z9e6+wd03RfPTgaZm1inOmEQkPuW11cXVhte6devE/JIlS/jNb37Dyy+/zPz58znhhBNS9tdv1qxZYr5x48YUFxen3Hbz5s13KlPVm5KVV75jx47Mnz+foUOHMnHiRC655BIAXnzxRS699FLefPNN8vLy2LZtW5XerybE2fvIgIeARe5+dzllOkflMLOBUTxr44pJROKVyTa8DRs20LZtW3bbbTdWrlzJiy++WOPvMXToUB577DEA3nnnnZRnIskGDRrEjBkzWLt2LcXFxUybNo1hw4ZRVFSEu/Od73yHW265hTlz5rBt2zYKCws55phjuPPOOykqKmJz2bq4WhBn76MhwHeBd8yspCVmHNAdwN1/B5wFXGZmxcAW4FzX/UFF6qySqtma7H2UrtzcXHr37k3fvn3p1asXQ4YMqfH3uOKKKzj//PPp168fubm59O3bN1H1k0q3bt249dZbGT58OO7OKaecwsknn8ycOXO46KKLcHfMjNtvv53i4mLOO+88Nm7cyPbt27n22mtp27ZtjX+GytS5ezTn5eW5brIjUnsWLVrEQQcdlOkwskJxcTHFxcW0aNGCJUuWcPzxx7NkyRKaNMmuwSFS/c3MbLa751X22uz6JCIiWWzTpk0ce+yxFBcX4+48+OCDWZcQqqt+fRoRkRi1b9+e2bNnZzqMWNX7sY9ERCR9SgoiIpKgpCAiIglKCiIikqCkICJZbfjw4TtdiDZhwgS+//3vV/i6Nm3aAPDJJ59w1llnlbvtyrq4T5gwYYeLyE466STWrVuXTugVuvnmm7nrrruqvZ2apqQgIlltxIgRTJs2bYdl06ZNY8SIEWm9fq+99uLxxx/f5fcvmxSmT59O+/btd3l72U5JQUSy2llnncWzzz7LV199BcDy5cv55JNPGDp0aOK6gdzcXA4++GCefvrpnV6/fPly+vbtC8CWLVs499xz6devH+eccw5btmxJlLvssssSw27fdNNNAEycOJFPPvmEo48+mqOPPhqAnJwc1qxZA8Ddd99N37596du3b2LY7eXLl3PQQQfxv//7v/Tp04fjjz9+h/dJZe7cuQwaNIh+/fpxxhln8Pnnnyfev3fv3vTr1y8xEN+//vWvxE2GBgwYwMaNG3d536ai6xREJG0//CHU9A3F+veH6HiaUseOHRk4cCAvvPACp512GtOmTeOcc87BzGjRogVPPvkku+22G2vWrGHQoEGceuqp5d6n+IEHHqBVq1bMnz+f+fPnk5ubm1g3fvx4dt99d7Zt28axxx7L/PnzufLKK7n77ruZMWMGnTrtOFbn7NmzmTx5MrNmzcLdOfzwwxk2bBgdOnRgyZIlTJ06ld///vecffbZPPHEExXeH+H888/n3nvvZdiwYdx4443ccsstTJgwgdtuu41ly5bRvHnzRJXVXXfdxf3338+QIUPYtGkTLVq0qMLerpzOFEQk6yVXISVXHbk748aNo1+/fhx33HF8/PHHrFq1qtztzJw5M3Fw7tevH/369Uuse+yxx8jNzWXAgAEsWLCg0sHuXnvtNc444wxat25NmzZtOPPMM3n11VcB6NmzJ/379wcqHp4bwv0d1q1bx7BhwwC44IILmDlzZiLGkSNHMmXKlMSV00OGDOGaa65h4sSJrFu3rsavqNaZgoikraJf9HE6/fTTueaaa5gzZw5btmxJ/MLPz8+nqKiI2bNn07RpU3JyclIOl50s1VnEsmXLuOuuu3jrrbfo0KEDo0ePrnQ7FY0bVzLsNoShtyurPirPc889x8yZM3nmmWf4+c9/zoIFCxg7diwnn3wy06dPZ9CgQfzjH//gwAMP3KXtp6IzBRHJem3atGH48OF873vf26GBef369ey55540bdqUGTNm8GGqG7InOeqoo8iP7g367rvvMn/+fCAMu926dWvatWvHqlWreP755xOvadu2bcp6+6OOOoqnnnqKzZs388UXX/Dkk09y5JFHVvmztWvXjg4dOiTOMh599FGGDRvG9u3b+eijjzj66KO54447WLduHZs2beL999/n4IMP5tprryUvL4/33nuvyu9ZEZ0piEidMGLECM4888wdeiKNHDmSU045hby8PPr371/pL+bLLruMCy+8kH79+tG/f38GDhwIhLuoDRgwgD59+uw07PaYMWM48cQT6dKlCzNmzEgsz83NZfTo0YltXHzxxQwYMKDCqqLyPPLII1x66aVs3ryZXr16MXnyZLZt28aoUaNYv3497s7VV19N+/btueGGG5gxYwaNGzemd+/eibvI1RQNnS0iFdLQ2XVPdYbOVvWRiIgkKCmIiEiCkoKIVKquVTM3ZNX9WykpiEiFWrRowdq1a5UY6gB3Z+3atdW6oE29j0SkQt26daOwsJCioqJMhyJpaNGiBd26ddvl1yspiEiFmjZtSs+ePTMdhtQSVR+JiEiCkoKIiCQoKYiISIKSgoiIJCgpiIhIgpKCiIgkKCmIiEiCkoKIiCQoKYiISIKSgoiIJCgpiIhIQmxJwcz2NrMZZrbIzBaY2VUpypiZTTSzpWY238xy44pHREQqF+eAeMXAj9x9jpm1BWab2UvuvjCpzInAftF0OPBA9CgiIhkQ25mCu6909znR/EZgEdC1TLHTgD958B+gvZl1iSsmERGpWK20KZhZDjAAmFVmVVfgo6TnheycODCzMWZWYGYFGtNdRCQ+sScFM2sDPAH80N03lF2d4iU73d7J3Se5e5675+2xxx5xhCkiIsScFMysKSEh5Lv7/6UoUgjsnfS8G/BJnDGJiEj54ux9ZMBDwCJ3v7ucYs8A50e9kAYB6919ZVwxiYhIxeLsfTQE+C7wjpnNjZaNA7oDuPvvgOnAScBSYDNwYYzxiIhIJWJLCu7+GqnbDJLLOPCDuGIQEZGq0RXNIiKSoKQgIiIJSgoiIpKgpCAiIglKCiIikqCkICIiCUoKIiKSoKQgIiIJSgoiIpKgpCAiIglKCiIikqCkICIiCUoKIiKSoKQgIiIJSgoiIpLQYJLChg3wwAPgO90BWkRESjSYpPDUU/D978OTT2Y6EhGR7NVgksJ558GBB8L118O2bZmORkQkOzWYpNCkCfziF7BoETz6aKajERHJTg0mKQCceSbk5cFNN8FXX2U6GhGR7NOgkoIZ/PKXsGIFPPhgpqMREck+DSopABx3HBx9dKhK2rQp09GIiGSXBpcUSs4WiopgwoRMRyMikl0aXFIAGDQITjsN7rwT1q7NdDQiItmjQSYFCNVHGzfC7bdnOhIRkezRYJNC374wahTcey98/HGmoxERyQ4NNikA3HJLuJDt5z/PdCQiItmhQSeFnj1hzBh46CFYujTT0YiIZF6DTgoQhr1o1gxuvDHTkYiIZF6DTwqdO8NVV8HUqTBvXqajERHJrAafFAB+8hNo3x6uuy7TkYiIZFZsScHMHjaz1Wb2bjnrh5vZejObG00Zq8Dp0AGuvRaeew5efz1TUYiIZF6cZwp/BE6opMyr7t4/mm6NMZZKXXllqEr62c90Ix4RabhiSwruPhP4LK7t17RWreCGG+DVV+GFFzIdjYhIZmS6TWGwmc0zs+fNrE+GY+Hii0M31XHjYPv2TEcjIlL7MpkU5gA93P0Q4F7gqfIKmtkYMysws4KioqLYAmrWDG69FebOhb/+Nba3ERHJWhlLCu6+wd03RfPTgaZm1qmcspPcPc/d8/bYY49Y4xoxIgyBccMNsHVrWJafDzk50KhReMzPjzUEEZGMaZKpNzazzsAqd3czG0hIUBkfs7RxYxg/Poyi+sc/hraGMWNg8+aw/sMPw3OAkSMzFqaISCzMY+pqY2ZTgeFAJ2AVcBPQFMDdf2dmlwOXAcXAFuAad3+jsu3m5eV5QUFBLDGXcIchQ8Id2ho1go8+2rlMjx6wfHmsYYiI1Bgzm+3ueZWWiyspxKU2kgLAK6+EO7SVx0yN0SJSd6SbFDLd+yhrDR8Oxx8fzhRS6d69VsMREakVaSUFM9vHzJpH88PN7Eozax9vaJn3y1+Gs4EmZVpeWrUK7Q4iIvVNumcKTwDbzGxf4CGgJ/Dn2KLKEoceCmedFZJCt26hyqhHD5g0SY3MIlI/pZsUtrt7MXAGMMHdrwa6xBdW9vj5z+Hrr0Ny2L49NC4rIYhIfZVuUthqZiOAC4Bno2VN4wkpuxx4IIweDb/9beiNJCJSn6WbFC4EBgPj3X2ZmfUEpsQXVna56abweMstmY1DRCRuaSUFd1/o7le6+1Qz6wC0dffbYo4ta3TvDt//friY7dlnKy0uIlJnpdv76BUz283MdgfmAZPN7O54Q8suN94IAwbA6afDww9nOhoRkXikW33Uzt03AGcCk939UOC4+MLKPh06wIwZcMwxcNFFobtqHbvuT0SkUukmhSZm1gU4m9KG5ganbdtQfXTeeeHWnVdeCdu2ZToqEZGak+6AeLcCLwKvu/tbZtYLWBJfWNmrWTN49NFwl7a774ZVq8Lz5s0zHZmISPWllRTc/a/AX5OefwB8O66gsl2jRvDrX8Nee8GPfwxFRfDUU9CuXaYjExGpnnQbmruZ2ZNmttrMVpnZE2bWLe7gst2PfhTOEl57DYYNg5UrMx2RiEj1pNumMBl4BtgL6Ar8LVrW4I0aFdoZli6FI46A//430xGJiOy6dJPCHu4+2d2Lo+mPQLy3QKtD/ud/Qs+kTZvCfRjeeivTEYmI7Jp0k8IaMxtlZo2jaRRZcJe0bHLYYfDGG6GH0vDh8MILmY5IRKTq0k0K3yN0R/0UWAmcRRj6QpLst19IDPvvD6ecEtobRETqknSHuVjh7qe6+x7uvqe7n064kE3K6NwZ/vUvOOooOP98uOuuTEckIpK+6tx57Zoai6Ke2W03mD4dzj4bfvKT0EtJt+4Ukbog3YvXUrEai6Ieat4cpk4tvcjt009h8uRw8ZuISLaqTlLQyD+VaNQIJkyALl3gZz+D1avDXdt69sx0ZCIiqVVYfWRmG81sQ4ppI+GaBamEGYwdG84SZs4MjdGjR8PixZmOTERkZxUmBXdv6+67pZjaunt1zjIanNGj4YMP4Ior4LHH4KCD4JxzYN68TEcmIlKqOg3NUkVdu8I994T7PI8dC88/D/37w6mnwqxZmY5ORERJoVbk50NOTmhjyMmBl14K92P48EO49VZ4/XUYNAi++c3QnVX3aRCRTFFSiFl+PowZExKAe3gcMyYs79ABbrghnDnceSe88064GvrII8MV0UoOIlLblBRidt11sHnzjss2bw7LS7RtG4bgXrYM7rsPVqyAE08MQ2c8+aSucRCp67Zvh0WLwv94cXGmo6mYeR37OZqXl+cFBQWZDiNtjRql/sVvVv7B/uuvYcoU+NWvwuirffrAuHHhYrgmat4XyXrusGQJvPxymGbMgDVrwrrGjaF799A1vVev0seS+U6dwvGhppnZbHfPq7SckkK8cnJClVFZPXqEaqOKFBfDX/8K48fDggWw777w05+G4TN0pzeR7LJiRWkSePll+PjjsLxr13Bv92HDwvNly0JPxJLH1at33E6bNjsnjOTHli13LT4lhSxR0qaQXIXUqlW4iG3kyPS2sX07PPNMSA4FBeGObz/6UdhumzbxxC0iFVu1KpwBlCSB998Pyzt1gqOPDongmGPCtUkV/fL/4ouQIJKTRfL8F1+Ulr366jBCwq5QUsgi+fmhDWHFinDaOH58+gkhmTv84x+hWmnGDNh993DdwxVXQMeONR+3iATuofrn9ddLk8CCBWHdbruFDiIlSaBPn1BtXFPvW1RUmiT22w/yKj2sp6akUM/95z9w223w9NPQujVccglcc004VRWRqtm8GT76KEwrVqSeL/nF3rJl6CFYkgQGDKgbbX0ZTwpm9jDwLWC1u/dNsd6A3wAnAZuB0e4+p7LtKins6N134fbbw+B7jRuH9oaf/jT8ohCpD7ZsgVdeCWfJW7ZA06ZhYMmyj6mWJa9r0iTU36c64K9Nccuwzp3Dmf3ee5dOhx0GAwfWzTa9bEgKRwGbgD+VkxROAq4gJIXDgd+4++GVbVdJIbVly8K1Dg8/DFu3wne+E66a7t8/05GJVN2KFfDcc2F6+eWQDFq0CN23t24NPfS2bg3TrmjffscDftn5rl3r34jGGU8KURA5wLPlJIUHgVfcfWr0fDEw3N1XVrRNJYWKffppGJn1t7+FjRvD9Q4/+1k43RXJVsXF4a6F06eHRPDuu2F5r15w8slw0kmh3r5Fix1f516aHEoSxddf7zif/NipUzjwN8QOGukmhUzWhHUFPkp6Xhgt2ykpmNkYYAxA9+7dayW4uqpz59DWMHYs3H9/SBBHHQVDh8K118Jxx+38jyWSCUVF4cr9556DF1+EdetCFc+RR4Y7Fp58MhxwQMU9d8xKq4dat6692OuzTCaFVH/qlKct7j4JmAThTCHOoOqL9u1Dj6err4aHHgpVS6ecEv558vLgiCNgyJDwuOeemY5WGgJ3ePvtkASmTw+DQLrDN74Bp58eksA3vwnt2mU60oZN1UcNxNdfh19lr70WutUVFIRlEC6KGzKkdDrwwJrrUid12+efw8KFYXrvPdi0qfyqmfIeS+a/+AI2bAjbPeywkAROPhlyc/V9qw11ofroGeByM5tGaGheX1lCkF3XrFkYovvUU8Pzr76C2bNDgnj99fDr7ZFHwroOHWDw4NIkcdhh4YI7qb8++yz0u1+4sPRx4UJYmfQf2bJl6JNfUS+fiso0axa+SyeeGM4OJDvF2ftoKjAc6ASsAm4CmgK4+++iLqn3AScQuqRe6O6VngLoTCEeJWO1vPFGaaJYtCisa9Ik9MU+4gjo1y9cnNO7d+gJUpvxbd6seuPqWrNm5wP/ggXh6twSrVuHv2/J37nksXt3/aKvy7Ki91EclBRqz9q14SK5kiTx1luha2CJHj3CASN5OuigXT9wu4dfpu+/HwYCLDtt2BB6jgweHKYjjghdbutb18Hq+uqrsA//+98wLV5c+lhUVFqubdtwsC+bALp108G/PlJSkBq3bVu41H7Bgh2n994rbZ8wC4MA9u27Y7I48MBQtbBtGxQWhoN82YP/++/vOEZUkyZhW/vuG6bOnUNXxTfeCP3YIfSkyssrTRKDB9du1cT27SFZrV+/87Ru3Y7PN20KXSF3373iKZ3eYdu3hwHXSg74yQf/5ct3HIG3c2fYf/8wHXRQaQLo1i2e0TglOykpSK0pLg4H9bLJYvHi0rHjGzUKA/mtXl2aQCBcGbrPPmEqOfiXTN27lz98wMcfw7//HaY33oA5c0q327NnaYI44gg4+ODKhyHYujWcGa1ZU/qYPL92bZjWrdvxYL9xY+X7p1lL+O/ZAAAMw0lEQVSz0KOmTZvQ2PrZZxWPqd+y5Y5JomPH8Ni2bUioixeHqr7ks7Y2bUoP/PvvH7py7r9/uLJdvXkElBTqlZoaUK+2ff11OHiVJIlly6BLlx0P/F271kxVxZdfhu6Ob7xRmihKGklbtw5DE+Tl7XjwTz7ol/SKSaV163DRU8eOoatvu3Y7TqmWJU+pLrjatCkkh6pM69eH/ZXq4N+li371S8WUFOqJmhh6uyFyD0k0OUnMmxcO0J06lR7kkx9TzXfsqIv9pH5QUqgnqnOTHtmRu35NS8OVblJQH4MsV9Kgmu5yKZ8SgkjllBSyXHlDPWkIKBGJg5JClhs/fueriVu1CstFRGqakkKWGzkyNCr36BGqP3r0UCOziMSnDtxETkaOVBIQkdqhMwUREUlQUhARkQQlBRERSVBSEBGRBCUFERFJUFJoAPLzw3AZjRqFx/z8TEckItlKXVLrubID6n34YXgO6uYqIjvTmUI9d911O46wCuH5dddlJh4RyW5KCvWcBtQTkapQUqjnNKCeiFSFkkI9pwH1RKQqlBTqOQ2oJyJVod5HDYAG1BORdOlMQUREEpQUREQkQUlBREQSlBRERCRBSUHSovGTRBoG9T6SSmn8JJGGQ2cKUimNnyTScCgpSKU0fpJIw6GkIJXS+EkiDUesScHMTjCzxWa21MzGplg/2syKzGxuNF0cZzyyazR+kkjDEVtSMLPGwP3AiUBvYISZ9U5R9C/u3j+a/hBXPLLrNH6SSMMRZ++jgcBSd/8AwMymAacBC2N8T4mJxk8SaRjirD7qCnyU9LwwWlbWt81svpk9bmZ7p9qQmY0xswIzKygqKoojVomZrnMQqRviTAqWYpmXef43IMfd+wH/AB5JtSF3n+Tuee6et8cee9RwmBK3kuscPvwQ3Euvc1BiEMk+cSaFQiD5l3834JPkAu6+1t2/ip7+Hjg0xngkQ3Sdg0jdEWdSeAvYz8x6mlkz4FzgmeQCZtYl6empwKIY45EM0XUOInVHbA3N7l5sZpcDLwKNgYfdfYGZ3QoUuPszwJVmdipQDHwGjI4rHsmc7t1DlVGq5SKSXcy9bDV/dsvLy/OCgoJMhyFVUHbsJAjXOahbq0jtMbPZ7p5XWTld0Syx03UOInWHRkmVWqHrHETqBp0pSJ2g6xxEaofOFCTr6X4OIrVHZwqS9XSdg0jtUVKQrKfrHERqj5KCZD3dz0Gk9igpSNarifs5qKFaJD1KCpL1qnudgwbkE0mfrmiWei8nJ/UwGz16wPLltR2NSGboimaRiBqqRdKnpCD1Xk00VKtNQhoKJQWp96rbUK02CWlIlBSk3qtuQ7UunpOGRElBGoSRI0Oj8vbt4bEqw2PURJuEqp+krlBSEKlEddskVP0kdYmSgkglqtsmoeonqUuUFEQqUd02CVU/SV2ipCCShuq0SWRD9ZOSiqRLSUEkZpmuflJSkapQUhCJWaarn7IhqUjdoaQgUgsyWf2U6aQC1T/T0JlK7VFSEMly1a1+ynRSqe6ZRjZUfzWopOTudWo69NBDXaShmTLFvUcPd7PwOGVK1V7bqpV7OKSGqVWr9LfRo8eOry2ZevSoG6+v7uev7utLtrGrf7+aeL27O1DgaRxjM36Qr+qkpCBSdZlMKmapD+pmtfN6JaUg3aSg+ymISKXy80MbwooVodpp/Pj020Wqez+L6r6+UaNwKC3LLLTxxP36TH/+ErqfgojUmOo0lFe3TSTTbSqZbpOp7fuBKCmISKyq2yW3uq9v6EmpytKpY8qmSW0KIlJVmWzoVZtCzNSmICJ1TXXaZGri9ZB+m4KSgohIA6CGZhERqbJYk4KZnWBmi81sqZmNTbG+uZn9JVo/y8xy4oxHREQqFltSMLPGwP3AiUBvYISZ9S5T7CLgc3ffF7gHuD2ueEREpHJxnikMBJa6+wfu/jUwDTitTJnTgEei+ceBY83MYoxJREQqEGdS6Ap8lPS8MFqWsoy7FwPrgY5lN2RmY8yswMwKioqKYgpXRESaxLjtVL/4y3Z1SqcM7j4JmARgZkVmluKi76zQCViT6SAqkO3xQfbHqPiqR/FVT3Xi65FOoTiTQiGwd9LzbsAn5ZQpNLMmQDvgs4o26u571GSQNcnMCtLp8pUp2R4fZH+Miq96FF/11EZ8cVYfvQXsZ2Y9zawZcC7wTJkyzwAXRPNnAS97XbtwQkSkHontTMHdi83scuBFoDHwsLsvMLNbCZdbPwM8BDxqZksJZwjnxhWPiIhULs7qI9x9OjC9zLIbk+a/BL4TZwy1bFKmA6hEtscH2R+j4qsexVc9scdX54a5EBGR+GiYCxERSVBSEBGRBCWFKjKzvc1shpktMrMFZnZVijLDzWy9mc2NphtTbSvGGJeb2TvRe+80pKwFE6Mxp+abWW4txnZA0n6Za2YbzOyHZcrU+v4zs4fNbLWZvZu0bHcze8nMlkSPHcp57QVRmSVmdkGqMjHFd6eZvRf9DZ80s/blvLbC70OM8d1sZh8n/R1PKue1FY6RFmN8f0mKbbmZzS3ntbHuv/KOKRn7/qVz0wVNpRPQBciN5tsC/wV6lykzHHg2gzEuBzpVsP4k4HnCxYODgFkZirMx8CnQI9P7DzgKyAXeTVp2BzA2mh8L3J7idbsDH0SPHaL5DrUU3/FAk2j+9lTxpfN9iDG+m4Efp/EdeB/oBTQD5pX9f4orvjLrfw3cmIn9V94xJVPfP50pVJG7r3T3OdH8RmAROw/fke1OA/7kwX+A9mbWJQNxHAu87+4Zv0Ld3Wey84WTyWNzPQKcnuKl/wO85O6fufvnwEvACbURn7v/3cPwMAD/IVwgmhHl7L90pDNGWrVVFF803trZwNSaft90VHBMycj3T0mhGqKhvgcAs1KsHmxm88zseTPrU6uBhaFC/m5ms81sTIr16YxLVRvOpfx/xEzuvxLfcPeVEP5xgT1TlMmWffk9wtlfKpV9H+J0eVS99XA51R/ZsP+OBFa5+5Jy1tfa/itzTMnI909JYReZWRvgCeCH7r6hzOo5hCqRQ4B7gadqObwh7p5LGLb8B2Z2VJn1aY05FafoKvdTgb+mWJ3p/VcV2bAvrwOKgfxyilT2fYjLA8A+QH9gJaGKpqyM7z9gBBWfJdTK/qvkmFLuy1Isq9b+U1LYBWbWlPDHy3f3/yu73t03uPumaH460NTMOtVWfO7+SfS4GniScIqeLJ1xqeJ2IjDH3VeVXZHp/ZdkVUm1WvS4OkWZjO7LqGHxW8BIjyqZy0rj+xALd1/l7tvcfTvw+3LeN9P7rwlwJvCX8srUxv4r55iSke+fkkIVRfWPDwGL3P3ucsp0jsphZgMJ+3ltLcXX2szalswTGiPfLVPsGeD8qBfSIGB9yWlqLSr311km918ZyWNzXQA8naLMi8DxZtYhqh45PloWOzM7AbgWONXdN5dTJp3vQ1zxJbdTnVHO+6YzRlqcjgPec/fCVCtrY/9VcEzJzPcvrhb1+joBQwmnZ/OBudF0EnApcGlU5nJgAaEnxX+AI2oxvl7R+86LYrguWp4cnxHuivc+8A6QV8v7sBXhIN8uaVlG9x8hQa0EthJ+fV1EuLfHP4El0ePuUdk84A9Jr/0esDSaLqzF+JYS6pNLvoe/i8ruBUyv6PtQS/E9Gn2/5hMOcF3Kxhc9P4nQ4+b92owvWv7Hku9dUtla3X8VHFMy8v3TMBciIpKg6iMREUlQUhARkQQlBRERSVBSEBGRBCUFERFJUFIQiZjZNttxBNcaG7HTzHKSR+gUyVax3o5TpI7Z4u79Mx2ESCbpTEGkEtF4+reb2ZvRtG+0vIeZ/TMa8O2fZtY9Wv4NC/c3mBdNR0Sbamxmv4/GzP+7mbWMyl9pZguj7UzL0McUAZQURJK1LFN9dE7Sug3uPhC4D5gQLbuPMAR5P8JgdBOj5ROBf3kY0C+XcCUswH7A/e7eB1gHfDtaPhYYEG3n0rg+nEg6dEWzSMTMNrl7mxTLlwPHuPsH0cBln7p7RzNbQxi6YWu0fKW7dzKzIqCbu3+VtI0cwrj3+0XPrwWauvsvzOwFYBNhNNinPBoMUCQTdKYgkh4vZ768Mql8lTS/jdI2vZMJY1EdCsyORu4UyQglBZH0nJP0+O9o/g3CqJ4AI4HXovl/ApcBmFljM9utvI2aWSNgb3efAfwUaA/sdLYiUlv0i0SkVEvb8ebtL7h7SbfU5mY2i/BDakS07ErgYTP7CVAEXBgtvwqYZGYXEc4ILiOM0JlKY2CKmbUjjF57j7uvq7FPJFJFalMQqUTUppDn7msyHYtI3FR9JCIiCTpTEBGRBJ0piIhIgpKCiIgkKCmIiEiCkoKIiCQoKYiISML/A0o+ZSjxXmvoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 2s 258us/step - loss: 2.5392 - acc: 0.5227 - val_loss: 1.6743 - val_acc: 0.6520\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 1.3711 - acc: 0.7120 - val_loss: 1.2760 - val_acc: 0.7200\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 1.0130 - acc: 0.7789 - val_loss: 1.1319 - val_acc: 0.7530\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 0.7975 - acc: 0.8247 - val_loss: 1.0553 - val_acc: 0.7590\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 0.6390 - acc: 0.8624 - val_loss: 0.9761 - val_acc: 0.7930\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.5118 - acc: 0.8915 - val_loss: 0.9107 - val_acc: 0.8130\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 2s 204us/step - loss: 0.4119 - acc: 0.9149 - val_loss: 0.8928 - val_acc: 0.8210\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 2s 206us/step - loss: 0.3353 - acc: 0.9288 - val_loss: 0.8737 - val_acc: 0.8290\n",
      "2246/2246 [==============================] - 1s 298us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9845023581945567, 0.7831700801424755]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:\n",
    "下面展示了随机猜测所有样本的类时，猜测正确的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1834372217275156"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:表示概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "十分重要！！！\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ce7cdb9b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* 在单标签多类分类问题中，您的网络应该以SOFTMax激活结束，以便它将在N个输出类上输出概率分布。\n",
    "* _Categorical crossentropy_ 几乎总是你用来解决这些问题的损失函数。它最小化了由网络输出的概率分布与目标的真实分布之间的距离。\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
